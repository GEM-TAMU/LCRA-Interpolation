{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.crs import CRS\n",
    "import gstools as gs\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from scipy.interpolate import Rbf\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(data_path, latlon_path, start_date, end_date):\n",
    "    # Initialize an empty dictionary to store dataframes and a list for site names\n",
    "    data_dict = {}    \n",
    "    common_index = pd.date_range(start=start_date, end=end_date)\n",
    "    site_name = []\n",
    "\n",
    "    # Iterate through each gauge directory in data_path\n",
    "    for gauge in data_path:\n",
    "        # Iterate through each file in the current gauge directory\n",
    "        for file_name in os.listdir(gauge):\n",
    "            # Append the file name to site_name list\n",
    "            site_name.append(file_name.split(\".xlsx\")[0])\n",
    "            file = os.path.join(gauge, file_name)\n",
    "            \n",
    "            # Read the Excel file into a dataframe\n",
    "            df = pd.read_excel(file)\n",
    "            \n",
    "            # Check if the dataframe contains a \"Date\" column\n",
    "            if \"Date\" in df.columns:\n",
    "                # Convert \"Date\" column to datetime and set it as the index\n",
    "                df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "                df.set_index(\"Date\", inplace=True)\n",
    "                \n",
    "                # Read latitude and longitude data from latlon_path\n",
    "                lat_lon_df = pd.read_excel(latlon_path)\n",
    "                lat_lon_df.set_index(\"site_number\", inplace=True)\n",
    "                \n",
    "                # Extract site number from the file name\n",
    "                site_number = int(file_name.split(\"_\")[0])\n",
    "                \n",
    "                # Get latitude and longitude for the current site number\n",
    "                lat_lon = lat_lon_df.loc[site_number]\n",
    "                latitude = float(lat_lon[\"Lat\"])\n",
    "                longitude = float(lat_lon[\"Long_\"])\n",
    "                lat_lon_tup = (latitude, longitude)\n",
    "                \n",
    "                # Reindex the dataframe to match the common date range\n",
    "                df = df.reindex(common_index)\n",
    "                df.reset_index(inplace=True)\n",
    "                \n",
    "                # Add the dataframe to the dictionary with the lat/lon tuple as the key\n",
    "                data_dict[lat_lon_tup] = df\n",
    "            else:\n",
    "                # Rename columns if the dataframe does not have a \"Date\" column\n",
    "                df.rename(columns={'DATE': 'Date', 'PRCP': 'Daily Rain (inches)', 'PRCP (in)': 'Daily Rain (inches)'}, inplace=True)\n",
    "                \n",
    "                # Convert \"Date\" column to datetime and set it as the index\n",
    "                df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "                df.set_index(\"Date\", inplace=True)\n",
    "                \n",
    "                # Extract latitude and longitude from the first row\n",
    "                lat_lon = df.iloc[0]\n",
    "                latitude = float(lat_lon[\"LATITUDE\"])\n",
    "                longitude = float(lat_lon[\"LONGITUDE\"])\n",
    "                \n",
    "                # Drop unnecessary columns\n",
    "                columns_to_delete = df.columns[:5]\n",
    "                df.drop(columns=columns_to_delete, inplace=True)\n",
    "                lat_lon_tup = (latitude, longitude)\n",
    "                \n",
    "                # Reindex the dataframe to match the common date range\n",
    "                df = df.reindex(common_index)\n",
    "                df.reset_index(inplace=True)\n",
    "                \n",
    "                # Add the dataframe to the dictionary with the lat/lon tuple as the key\n",
    "                data_dict[lat_lon_tup] = df\n",
    "    \n",
    "    # Return the dictionary of dataframes and the list of site names\n",
    "    return data_dict, site_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prcp_path_hydromet = \"/home/arvinder/interpolation/Precipitation/Hydromet\"\n",
    "hydromet_latlon_path = \"/home/arvinder/interpolation/Precipitation/Hydromet Gage LatLong.xls\"\n",
    "\n",
    "prcp_path_NOAA = \"/home/arvinder/interpolation/Precipitation/NOAA\"\n",
    "NOAA_latlon_path = \"/home/arvinder/interpolation/Precipitation/NOAA Gage LatLong.xlsx\"\n",
    "LCRA_shp_pth = \"/home/arvinder/interpolation/HUC4_1209/HUC4_1209.shp\"\n",
    "\n",
    "prism_shape_file_4km = \"/home/arvinder/Downloads/prism_4km_mesh/prism_4km_mesh.shp\"\n",
    "\n",
    "start_date = '2003-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "# prcp_dict, site_names = get_dataframe([prcp_path_hydromet, prcp_path_NOAA], hydromet_latlon_path)\n",
    "prcp_dict, site_names = get_dataframe([prcp_path_hydromet], hydromet_latlon_path, start_date, end_date)\n",
    "prcp_coord = list(prcp_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data- Time distribution \n",
    "\n",
    "def data_time_distribution(prcp_dict):\n",
    "    # Reindex dataframe with time indexing\n",
    "    all_df = prcp_dict.values()\n",
    "    all_df = [df.rename(columns={df.columns[0]: 'time'}) for df in all_df]\n",
    "    all_df = [df.set_index('time') for df in all_df]\n",
    "\n",
    "    # Drop nan values, Add (start, end) date to the list and then sort it on the basis of start date\n",
    "    start_end = []\n",
    "    for df in all_df:\n",
    "        df = df.dropna()\n",
    "        start_end.append((df.index.min(), df.index.max()))\n",
    "    start_end = sorted(start_end, key=lambda x: x[0])\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(15, 10), dpi=100)\n",
    "\n",
    "    # Plot each interval as a horizontal bar\n",
    "    for i, (start, end) in enumerate(start_end):\n",
    "        ax.barh(i, (end - start).days, left=start, height=0.4, align='center')\n",
    "\n",
    "\n",
    "    # Format the x-axis to show dates nicely\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())  # Set the major ticks to every 5 years\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_title('Time Intervals')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "data_time_distribution(prcp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid_and_plot(LCRA_shp_pth, prism_shape_file):\n",
    "\n",
    "    #Read and Reproject the world and grid map to EPSG:3857 (meters)\n",
    "    world = gpd.read_file(LCRA_shp_pth)\n",
    "    world = world.to_crs(epsg=3857)\n",
    "\n",
    "    mesh_grid = gpd.read_file(prism_shape_file)\n",
    "    mesh_grid = mesh_grid.to_crs(epsg=3857)\n",
    "\n",
    "    # Perform the intersection\n",
    "    intersected_mesh = gpd.sjoin(mesh_grid, world, how='inner', predicate='intersects')\n",
    "\n",
    "    # Reproject the world and grid map to EPSG:4326\n",
    "    intersected_mesh = intersected_mesh.to_crs(epsg=4326)\n",
    "    world = world.to_crs(epsg=4326)\n",
    "\n",
    "    #Plot world, mesh and prcp data\n",
    "    base = world.plot(color='lightgray', figsize=(10, 6))\n",
    "    intersected_mesh.boundary.plot(ax=base, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "    prcp_gdf = gpd.GeoDataFrame(geometry=[Point(lon, lat) for lat, lon in prcp_coord], crs=\"EPSG:4326\")\n",
    "    prcp_gdf.plot(ax=base, marker='o', color='red', markersize=2)\n",
    "\n",
    "    plt.title('Geographical Coordinates')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate the centroids using an equal-area projection\n",
    "    equal_area_crs = '+proj=cea'\n",
    "    intersected_mesh['centroid_equal_area'] = intersected_mesh.to_crs(equal_area_crs).centroid.to_crs(intersected_mesh.crs)\n",
    "\n",
    "    # Extract the coordinates of the centroids\n",
    "    centroid_equal_area = intersected_mesh['centroid_equal_area'].apply(lambda point: (point.x, point.y)).tolist()\n",
    "    # Extract x and y coordinates of centroids\n",
    "    centroid_coords = np.array(centroid_equal_area)\n",
    "\n",
    "    return centroid_coords, intersected_mesh\n",
    "\n",
    "test_data_coords, intersected_mesh = get_centroid_and_plot(LCRA_shp_pth, prism_shape_file_4km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "train_coords = np.array(list(prcp_dict.keys()))\n",
    "train_values = list(np.array(prcp_data['Daily Rain (inches)']) for prcp_data in prcp_dict.values())\n",
    "\n",
    "x_coords_train = np.array(train_coords[:, 1])   #  Longitude\n",
    "y_coords_train = np.array(train_coords[:, 0])   #  Latitude\n",
    "\n",
    "# Create an array of dates between start and end date\n",
    "dates = np.arange(start_date, end_date, dtype='datetime64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_kriging(dates, x_coords_train, y_coords_train, train_values, test_data_coords):\n",
    "    i = 0\n",
    "    interpolated_vals = []\n",
    "    interpolated_var = []\n",
    "\n",
    "    while i < len(dates):   #len(dates)\n",
    "\n",
    "        values = np.array([array[i] for array in train_values])\n",
    "\n",
    "        # Clean NaN and inf values\n",
    "        valid_indices = ~np.isnan(values)\n",
    "        x_coords_valid = np.array(x_coords_train)[valid_indices]\n",
    "        y_coords_valid = np.array(y_coords_train)[valid_indices]\n",
    "        values_valid = values[valid_indices]\n",
    "        \n",
    "        if len(values_valid) < 2:\n",
    "            # Not enough valid points to perform interpolation\n",
    "            interpolated_vals.append(np.full(test_data_coords.shape[0], np.nan))\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Define a spatiotemporal variogram model\n",
    "        model = gs.Exponential(dim=2, var=1.0, len_scale=0.9)\n",
    "\n",
    "        # Create the kriging field\n",
    "        krig = gs.krige.Krige(\n",
    "            model, cond_pos=[x_coords_valid, y_coords_valid], cond_val=values\n",
    "        )\n",
    "\n",
    "        # Perform the kriging\n",
    "        field, variance = krig((test_data_coords[:, 0], test_data_coords[:, 1]))\n",
    "\n",
    "        # Ensure non-negative values not present\n",
    "        field[field < 0] = 0\n",
    "        interpolated_vals.append(field)\n",
    "        interpolated_var.append(variance)\n",
    "        i+=1\n",
    "    return interpolated_vals, interpolated_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rbf(dates, x_coords_train, y_coords_train, train_values, test_data_coords):\n",
    "    i = 0\n",
    "    interpolated_vals = []\n",
    "    \n",
    "    while i < len(dates):   #len(dates)\n",
    "        # Extract the values for the current date\n",
    "        values = np.array([array[i] for array in train_values])\n",
    "\n",
    "        # Clean NaN and inf values\n",
    "        valid_indices = ~np.isnan(values)\n",
    "        x_coords_valid = np.array(x_coords_train)[valid_indices]\n",
    "        y_coords_valid = np.array(y_coords_train)[valid_indices]\n",
    "        values_valid = values[valid_indices]\n",
    "        \n",
    "        if len(values_valid) < 2:\n",
    "            # Not enough valid points to perform interpolation\n",
    "            interpolated_vals.append(np.full(test_data_coords.shape[0], np.nan))\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # Perform RBF interpolation using thin plate smoothing\n",
    "        rbf_interpolator = Rbf(x_coords_valid, y_coords_valid, values_valid)\n",
    "        \n",
    "        # Compute interpolated values at the centroid coordinates\n",
    "        field = rbf_interpolator(test_data_coords[:, 0], test_data_coords[:, 1])\n",
    "        \n",
    "        # Ensure non-negative values not present\n",
    "        field[field < 0] = 0\n",
    "        \n",
    "        interpolated_vals.append(field)\n",
    "        i += 1\n",
    "\n",
    "    return interpolated_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_vals = init_rbf(dates, x_coords_train, y_coords_train, train_values, test_data_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interpolated_data(day_to_plt, intersected_mesh, interpolated_vals):\n",
    "    # Plotting\n",
    "    _, ax = plt.subplots(figsize=(10, 8))\n",
    "    intersected_mesh.plot(ax=ax, facecolor='none', edgecolor='blue')  # Plot the original geometries for reference\n",
    "\n",
    "    # Plot centroids with color representing precipitation values\n",
    "    ax.scatter(test_data_coords[:, 0], test_data_coords[:, 1], c=interpolated_vals[day_to_plt], cmap='viridis', s=50, alpha=0.8, vmin=0)\n",
    "    plt.title('Precipitation at Centroids')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "def sample_points(x_coords, y_coords, values_orig, sample_size):\n",
    "\n",
    "    #  generate random array\n",
    "    random_arr = np.random.randint(len(x_coords) - 1, size=(int(len(x_coords) * sample_size)))\n",
    "\n",
    "    # Empty list to store random gauges\n",
    "    validation_x_coords = []\n",
    "    validation_y_coords = []\n",
    "    validation_values = []\n",
    "\n",
    "    # adding long/lat and values info to the empty list\n",
    "    for random_num in random_arr:\n",
    "        validation_x_coords.append(x_coords[random_num])\n",
    "        validation_y_coords.append(y_coords[random_num])\n",
    "        validation_values.append(values_orig[random_num])\n",
    "\n",
    "    # Create three new lists, excluding the elements at the specified indexes\n",
    "    train_x_coords = [item for idx, item in enumerate(x_coords) if idx not in random_arr]\n",
    "    train_y_coords = [item for idx, item in enumerate(y_coords) if idx not in random_arr]\n",
    "    train_values = [item for idx, item in enumerate(values_orig) if idx not in random_arr]\n",
    "\n",
    "    return validation_x_coords, validation_y_coords, validation_values, train_x_coords, train_y_coords, train_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_point(given_latlon, lat_list, lon_list):\n",
    "    closest_point_id = None\n",
    "    min_distance = float('inf')\n",
    "    \n",
    "    for loc_id, (lat, lon) in enumerate(zip(lat_list, lon_list)):\n",
    "        point = (lat, lon)\n",
    "        distance = geodesic(given_latlon, point).kilometers\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_point_id = loc_id    \n",
    "\n",
    "    return closest_point_id    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Randomly sample 20% points for validation\n",
    "sample_size = 0.2\n",
    "validation_x_coords, validation_y_coords, validation_values, train_x_coords, train_y_coords, train_values = sample_points(x_coords_train, y_coords_train, train_values, sample_size)\n",
    "\n",
    "# Perform Kriging and store new interpolated vals\n",
    "new_interpolated_vals = init_kriging(dates, train_x_coords, train_y_coords, train_values)\n",
    "\n",
    "# Find the closest point to the radomly sampled validation points\n",
    "closest_points = []\n",
    "for i in range(len(validation_x_coords)):\n",
    "    clst_pt = find_closest_point((validation_y_coords[i], validation_x_coords[i]), test_data_coords[:, 1], test_data_coords[:, 0])\n",
    "    closest_points.append(clst_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_analysis(sampled_interpolated_vals, validation_values, closest_points, r_sq_threshold):\n",
    "    i = 0\n",
    "    new_vals = np.vstack(sampled_interpolated_vals)\n",
    "    r_sq_list = []\n",
    "    mae_list = []\n",
    "    for ran_val in validation_values:\n",
    "        observed = ran_val[:-1]\n",
    "        predicted = new_vals[:, closest_points[i]]\n",
    "\n",
    "        # Filter out NaN values\n",
    "        valid_mask = ~np.isnan(observed)\n",
    "        observed_filtered = observed[valid_mask]\n",
    "        predicted_filtered = predicted[valid_mask]\n",
    "        i += 1\n",
    "        r_squared = r2_score(observed_filtered, predicted_filtered)\n",
    "        mae = mean_absolute_error(observed_filtered, predicted_filtered)\n",
    "\n",
    "        r_sq_list.append(r_squared)\n",
    "        mae_list.append(mae * 25.4)    # inch to mm\n",
    "    \n",
    "    num_val_stations = len(r_sq_list)\n",
    "    median_r_sqr = np.median(r_sq_list)\n",
    "    mean_r_sqr = np.mean(r_sq_list)\n",
    "\n",
    "    median_mae = np.median(mae_list)\n",
    "    mean_mae = np.mean(mae_list)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for r_sq in r_sq_list:\n",
    "        if r_sq > r_sq_threshold:\n",
    "            count += 1\n",
    "\n",
    "    num_above_threshold = count/num_val_stations\n",
    "\n",
    "    print(f\"Number of weather stations in validation: {num_val_stations}\")\n",
    "    print(f\"Median R square: {median_r_sqr}\")\n",
    "    print(f\"Mean R square: {mean_r_sqr}\")\n",
    "    print(f\"Median MAE (mm/day): {median_mae}\")\n",
    "    print(f\"Mean MAE (mm/day): {mean_mae}\")\n",
    "    print(f\"No. of stations above {r_sq_threshold} R-squared: {num_above_threshold}\")\n",
    "    return r_sq_list, mae_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(analyzed_data_list, type_of_data):\n",
    "    interval = 0.1\n",
    "    bin_edges = np.arange(min(analyzed_data_list) - interval, max(analyzed_data_list) + interval, interval)\n",
    "\n",
    "    # Create histogram\n",
    "    plt.hist(analyzed_data_list, bins=bin_edges, edgecolor='black')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(type_of_data)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Histogram of {type_of_data} with {interval} Intervals')\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rsq(intersected_mesh, r_sq_list, ):\n",
    "    # Plotting\n",
    "    _, ax = plt.subplots(figsize=(10, 8))\n",
    "    intersected_mesh.plot(ax=ax, facecolor='none', edgecolor='black')  # Plot the original geometries for reference\n",
    "\n",
    "    for idx, r_sw in enumerate(r_sq_list):\n",
    "        if r_sw > 0.5:\n",
    "            # Create the scatter plot\n",
    "            ax.scatter(validation_x_coords[idx], validation_y_coords[idx], color=\"green\")\n",
    "        if 0.3 < r_sw <= 0.5:\n",
    "            # Create the scatter plot\n",
    "            ax.scatter(validation_x_coords[idx], validation_y_coords[idx], color=\"yellow\")\n",
    "        if 0.3 >= r_sw :\n",
    "            # Create the scatter plot\n",
    "            ax.scatter(validation_x_coords[idx], validation_y_coords[idx], color=\"red\")\n",
    "        if r_sw < 0:\n",
    "            ax.scatter(validation_x_coords[idx], validation_y_coords[idx], color=\"Black\")\n",
    "\n",
    "    plt.title('Scatter Plot of R^2 Values with Custom Red-Yellow-Green Colormap')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Density vs different interpolation techniques\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(interpolated_vals)):\n",
    "    raster_data = {\n",
    "    \"latitude\": test_data_coords[:, 1],\n",
    "    \"longitude\": test_data_coords[:, 0],\n",
    "    \"value\": interpolated_vals[98]\n",
    "    }\n",
    "\n",
    "    # Convert to GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(raster_data, geometry=gpd.points_from_xy(raster_data['longitude'], raster_data['latitude']))\n",
    "\n",
    "    # Define the grid resolution (4km x 4km)\n",
    "    grid_size = 0.041666666667  # 4km in degrees\n",
    "\n",
    "    # Define the bounds of the raster\n",
    "    minx, miny, maxx, maxy = gdf.total_bounds\n",
    "    width = int(np.ceil((maxx - minx) / grid_size))\n",
    "    height = int(np.ceil((maxy - miny) / grid_size))\n",
    "\n",
    "    # Create an empty raster\n",
    "    raster = np.full((height, width), np.nan, dtype=np.float32)  # Initialize with NaNs to identify empty cells\n",
    "\n",
    "    # Calculate the transform\n",
    "    transform = from_origin(minx, maxy, grid_size, grid_size)\n",
    "    crs = CRS.from_epsg(4326)  # Assuming WGS84\n",
    "\n",
    "    # Fill the raster with values\n",
    "    for _, row in gdf.iterrows():\n",
    "        rowx = int((row.geometry.x - minx) / grid_size)\n",
    "        rowy = int((maxy - row.geometry.y) / grid_size)\n",
    "        if 0 <= rowx < width and 0 <= rowy < height:  # Ensure indices are within bounds\n",
    "            if np.isnan(raster[rowy, rowx]):\n",
    "                raster[rowy, rowx] = row['value']\n",
    "            else:\n",
    "                # Handle multiple points in the same cell; here we take the average\n",
    "                raster[rowy, rowx] = (raster[rowy, rowx] + row['value']) / 2\n",
    "        else:\n",
    "            print(\"Out of bounds\")\n",
    "        \n",
    "\n",
    "    # Define the output raster file\n",
    "    output_file = 'output'+ str(i) + '.tif'\n",
    "\n",
    "    # Write the raster to a file\n",
    "    with rasterio.open(\n",
    "        output_file,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=raster.shape[0],\n",
    "        width=raster.shape[1],\n",
    "        count=1,\n",
    "        dtype=raster.dtype,\n",
    "        crs=crs,  # WGS84 Latitude/Longitude\n",
    "        transform=transform,\n",
    "    ) as dst:\n",
    "        dst.write(raster, 1)\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
